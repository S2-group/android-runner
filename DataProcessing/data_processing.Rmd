---
title: "App vs Web App"
author: "Lucien Martijn, Stijn Meijerink, DaniÃ«l Hana, Terry Bommels, Dion David Haneveld"
date: "23 October 2020"
output: pdf_document
highlight: tango
---

```{r, results='hide'}
# Include libraries used in lectures
library(tidyverse)
library(car)
library(bestNormalize)
library(ggplot2)
library(effsize)
# D no ouput
```

First we have to read in the data. There were four seperate runs we have to 'stitch' together the aggregated results.
```{r}
# First aggregated read, do we need it?
# data <- read.csv('~/Git/uni/GL/android-runner/Data/EXAMPLE-Aggregated_Results_Batterystats.csv', sep = ';')
# data <- tibble(data)

# as.factor(data$device)
# as.factor(data$app_type)
# as.factor(data$subject)
# as.numeric(data$batterystats_Joule_calculated)
```

We also want to be sure that the data is valid, thus not having a too high standard deviation between subjects and treatments themselves.
```{r}
# Get all joule_results.*.csv
csv_paths <- list.files(path = './Data/', 
                        recursive = TRUE, 
                        pattern = '^Joule.*\\.csv', 
                        full.names = TRUE)
# read all csv
app_data <- csv_paths %>%
  lapply(read_csv) %>%
  bind_rows

# Find web apps based on www in their path
web_apps <- csv_paths %>%
  strsplit('-', fixed = TRUE) %>%
  rapply(nth, n=3) %>%
  str_detect(., 'www')
# Split into web and native app_type
temp_list <- list(length(web_apps))
for(i in 1:length(web_apps)){
  if(web_apps[i]){
    temp_list[i] = 'web'
  } else{
    temp_list[i] = 'native'
  }
}
app_data['app_type'] <- unlist(temp_list)
app_data$app_type <- as.factor(app_data$app_type)

# Add paths to data frame for reading out the correct names later
app_data['path'] = csv_paths
# tibble(app_data)

# Filter on https-www to get the web apps again
web_app_paths <- app_data %>%
  filter(str_detect(path, '(.*https-www.*)'))
# Split paths to get the correct name
web_app_names <- web_app_paths$path %>% 
  strsplit('-', TRUE) %>%
  rapply(nth, n=4)

# Filter on android-runner-experiments to get the native apps again
native_app_paths <- app_data %>%
  filter(str_detect(path, '(.*android-runner-experiments.*)'))
native_app_names <- native_app_paths$path %>% 
  strsplit('-', TRUE) %>%
  rapply(nth, n=7)

# Combine web app names and native app names and add them to the dataframe
app_names <- c(web_app_names, native_app_names)
app_data['app'] <- app_names 

updateMisalignedAppNames <- function(app){
  if(app == 'alibaba'){
    app = 'aliexpress'
  } else if(app == 'inditex'){
    app = 'zara'
  } else if(app == 'ninegag'){
    app = '9gag'
  } else if(app == 'google'){
    app = 'youtube'
  }else {
    app = app
  }
}

# The native app name for aliexpress is Alibaba so we update it to aliexpress
updatedAppNames <- app_data$app %>%
  lapply(updateMisalignedAppNames)

# Add the updated names to the dataframe again
app_data['app'] <- unlist(updatedAppNames)

# Based on the paths get the device tier and add it to the dataframe
app_data['device'] <- csv_paths %>%
  strsplit('-', fixed = TRUE) %>%
  rapply(nth, n=1) %>%
  strsplit('/', fixed = TRUE) %>%
  rapply(nth, n=6) %>%
  factor

# Show a part of the data frame
tail(app_data)
```

As we can see ....

# 1. Descriptive statistics
```{r}
summarize_data <- function(data){
  data %>%
  group_by(app) %>%
  summarize(n = n(), 
            mean = mean(Joule_calculated),
            median = median(Joule_calculated),
            sd = sd(Joule_calculated),
            IQR = IQR(Joule_calculated),
            min = min(Joule_calculated),
            max = max(Joule_calculated))
}

# Grouped by app type, so not split based on device tier
summarize_data(app_data)

# Summary for high-end device
high_end_app_data <- app_data %>%
  filter(device == 'high')
summarize_data(high_end_app_data)

high_end_web_app_data <- high_end_app_data %>%
  filter(app_type == 'web')
summarize_data(high_end_web_app_data)

high_end_native_app_data <- high_end_app_data %>%
  filter(app_type == 'native')
summarize_data(high_end_native_app_data)

# Summary for low-end device
low_end_app_data <- app_data %>%
  filter(device == 'low')
summarize_data(low_end_app_data)

low_end_web_app_data <- low_end_app_data %>%
  filter(app_type == 'web')
summarize_data(low_end_web_app_data)

low_end_native_app_data <- low_end_app_data %>%
  filter(app_type == 'native')
summarize_data(low_end_native_app_data)

```

```{r}
# For all devices
app_data$Joule_calculated %>%
  hist(breaks = 25, main = "Distribution of Joule_calculated for both device tiers and app types")

# For high-end
high_end_app_data$Joule_calculated %>%
  hist(breaks = 25, main = "Distribution of Joule_calculated for high-end with all app types")
# For low-end
low_end_app_data$Joule_calculated %>%
  hist(breaks = 25, main = "Distribution of Joule_calculated for low-end with all app types")
```


```{r}
check_normality <- function(data){
  plot(density(data))
  car::qqPlot(data)
  shapiro.test(data)
}

low_end_app_data$Joule_calculated %>%
  check_normality

high_end_app_data$Joule_calculated %>%
  check_normality

app_data$Joule_calculated %>%
  check_normality
```

```{r}
font_size <- 12

visualize <- function(app_data, title){
  result <- ggplot(app_data, aes(x = app, y = Joule_calculated)) +
  theme_bw() +
  xlab("App") + ylab("Joules") +
  ylim(c(0, 800)) +
  geom_violin(trim = TRUE, alpha = 0.5, width = 1) +
  geom_boxplot(color = 'red', width = .2, fill = 'white', outlier.size = .5, alpha = .6) +
  stat_summary(fun = mean, color = 'black', geom = 'point', shape = 5, size = 2, show.legend = T) +
  theme(
    strip.text.x = element_text(size = font_size),
    strip.text.y = element_text(size = font_size),
    axis.text.x = element_text(size = font_size),
    axis.text.y = element_text(size = font_size)
    ) + 
  labs(title = title, subtitle = NULL, caption = NULL)

  
  return(result)
}
visualize(high_end_web_app_data, title = "Web subjects' energy consumption for high-end device")
visualize(high_end_native_app_data, title = "Native subjects' energy consumption for high-end device")
visualize(low_end_web_app_data, title = "Web subjects' energy consumption for low-end device")
visualize(low_end_native_app_data, title = "Native subjects' energy consumption for low-end device")
```

```{r}
apps <- c('9gag', 'aliexpress', 'deliveroo', 'reddit', 'weather', 'youtube', 'zara')
historgramsPerApp <- function(device_tier_app_type_data){
  for (appName in apps) {
     dataApp <- device_tier_app_type_data %>%
        filter(app == appName)
     hist(x = dataApp$Joule_calculated, main = paste("Histogram of joules for ", appName), breaks = 25)
  }  
}
historgramsPerApp(high_end_native_app_data)
historgramsPerApp(high_end_web_app_data)
historgramsPerApp(low_end_web_app_data)
```

```{r}
# Show joules per app type
plot(app_data$Joule_calculated ~ app_data$app_type)

# Show joules per app type
plot(app_data$Joule_calculated ~ app_data$device)
```


# 2. Select statistical test
Now that assessed normality we can select the statistical tests.

For RQ1 the hyptohesis were formulated as follows:
$H1_0 \tau_{web} = \tau_{native}$
$H1_1 \tau_{web} \neq \tau_{native}$
For RQ2 the hyptohesis were formulated as follows:
$H2_0 : \tau\beta_{ij} = 0\forall i,j$
$H2_1 : \exists(ij)|\tau\beta_{ij} \neq 0$

Both hyptotheses can be answered with a two way-anova if the following assumptions are met. The dependent variable is continous (energy consumption in joules). The samples are obtained indepently. 

The data follows a normal distribution as showed in the Descriptive statistics chapter.

Residuals follow normal distribution:
```{r}
# Two way ANOVA
app_data.two_way_aov <- aov(app_data$Joule_calculated ~ app_data$app_type * app_data$device)

qqPlot(residuals(app_data.two_way_aov))
```

Homoscedasticity (varaiance between groups should be equal) check:
```{r}
# Not too sure? leveneTest(x ~ y, data = myData) in slides
leveneTest(app_data$Joule_calculated, app_data$app_type)
```

# 3. Hypothesis testing
```{r}
summary(app_data.two_way_aov)
# TODO do we need to do p-value corrections?

# TODO do we need Tukey's test, this isn't correct with example data at least
postHoc <- TukeyHSD(x = app_data.two_way_aov, 'app_data$app_type', conf.level = 0.95)
plot(postHoc)
postHoc
```

```{r}
# If normality can't be assumed
# Filter data with blocking factor device tier
t.test.data <- app_data
# FIXME not correct need to check slides
# Paired t test 
t.test(t.test.data$Joule_calculated ~ t.test.data$app_type, data = app_data, paired = TRUE)
```

# 4. Effect size calculation
Now that we obtained our p-values we want to know what the effect size is. As shown before the results are significant. However the question remains is the difference in energy consumption in practice big enough to make a valid choice for the user and developer (described in the paper). 
As we used an ANOVA we use Cohen\'s measure.
```{r}
# This text above is only true when p-value is significant

# FIXME separate on device type?

# Get all batterystats_Joule_calculated where app_type == web
treatmentWeb <- app_data %>%
  filter(app_type == 'web')

# Get all batterystats_Joule_calculated where app_type == native
treatmentNative <- app_data %>%
  filter(app_type == 'native')

# FIXME doesn't work with example data yet
cohen.d(treatmentWeb$Joule_calculated, treatmentNative$Joule_calculated, paired=FALSE, pooled=FALSE)
# FIXME is paired false?
```

# 5. Power analysis (optional)
```{r}

```


